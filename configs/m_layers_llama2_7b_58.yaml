detach_head: False
model_name: 'meta-llama/llama-2-7b-hf'
tmp_path: 'path/llama2-7b-m-58'
decomposition: 'monarch'
layer_numbers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41]
q_proj:
    kl: 8
    bl1: 512
    bl2: 512
    kr: 2
    br1: 2048
    br2: 2048
    use_pl: False
    use_pr: False
k_proj:
    kl: 8
    bl1: 512
    bl2: 512
    kr: 2
    br1: 2048
    br2: 2048
    use_pl: False
    use_pr: False
v_proj:
    kl: 1
    bl1: 4096
    bl2: 4096
    kr: 4096
    br1: 1
    br2: 1
    use_pl: False
    use_pr: False
o_proj:
    kl: 2
    bl1: 2048
    bl2: 2048
    kr: 8
    br1: 512
    br2: 512
    use_pl: False
    use_pr: False
gate_proj:
    kl: 4
    bl1: 1024
    bl2: 1866
    kr: 4
    br1: 1866
    br2: 2752
    use_pl: False
    use_pr: False
up_proj:
    kl: 4
    bl1: 1024
    bl2: 1866
    kr: 4
    br1: 1866
    br2: 2752
    use_pl: False
    use_pr: False
down_proj:
    kl: 4
    bl1: 2752
    bl2: 1866
    kr: 4
    br1: 1866
    br2: 1024
    use_pl: False
    use_pr: False
lm_head:
    kl: 4096
    bl1: 1
    bl2: 1
    kr: 1
    br1: 4096
    br2: 32000
    use_pl: False
    use_pr: False
embed_tokens:
    kl: 1
    bl1: 32000
    bl2: 4096
    kr: 4096
    br1: 1
    br2: 1
    use_pl: False
    use_pr: False
not_replace_layers: 
